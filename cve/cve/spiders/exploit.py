import scrapy
import os

current_dir = os.path.dirname(__file__)
url = os.path.join(current_dir, 'source-EXPLOIT-DB.html')

class ExploitSpider(scrapy.Spider):
    name = 'exploit'
    allowed_domains = ['cve.mitre.org']
    # Starting with actual URLs is fine
    #start_urls = ['http://cve.mitre.org/data/refs/refmap/source-EXPLOIT-DB.html']
    # But you can use files as well!
    start_urls = [f"file://{url}"]

    def parse(self, response):
        table = None
        for child in response.xpath('//table'):
            if len(child.xpath('tr')) > 100:
                table = child
        for row in table.xpath('//tr'):
            cve_list = []
            try:
                # This captures 1 CVE only, but you may have many
                exploit_id = row.xpath('td//text()')[0].extract()
                cve_id = row.xpath('td//text()')[2].extract()
                print(f"exploit id: {exploit_id} -> {cve_id}")
#               # This is one way of doing that
#                for text in row.xpath('td//text()'):
#                    if text.extract().startswith('CVE'):
#                        cve_list.append(text.extract())
#                print(f"exploit id: {exploit_id} -> {cve_list}")
            except Exception as err:
                print(f"skipping due to: {err}")
